{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-3.20.1-cp38-cp38-win_amd64.whl (904 kB)\n",
      "Collecting emoji\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza) (1.19.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "Collecting torch>=1.3.0\n",
      "  Downloading torch-1.11.0-cp38-cp38-win_amd64.whl (158.0 MB)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza) (4.50.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza) (2.24.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza) (1.15.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers->stanza) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers->stanza) (2020.10.15)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers->stanza) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers->stanza) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza) (1.25.11)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers->stanza) (2.4.7)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171036 sha256=dd3bebe9a075761a1793c244481a94795d4b8ab88aee4438c9047fe8d1f9f901\n",
      "  Stored in directory: c:\\users\\vanis\\appdata\\local\\pip\\cache\\wheels\\5e\\8c\\80\\c3646df8201ba6f5070297fe3779a4b70265d0bfd961c15302\n",
      "Successfully built emoji\n",
      "Installing collected packages: protobuf, emoji, huggingface-hub, tokenizers, transformers, torch, stanza\n",
      "Successfully installed emoji-1.7.0 huggingface-hub-0.6.0 protobuf-3.20.1 stanza-1.4.0 tokenizers-0.12.1 torch-1.11.0 transformers-4.19.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "huggingface-hub 0.6.0 requires packaging>=20.9, but you'll have packaging 20.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d6d54862fa4fb79e799d203a22cafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/res…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 11:24:27 INFO: Downloading default packages for language: en (English)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02eca7f55dd44fe1aa40968b6b178b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.0/models/defa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 11:24:52 INFO: Finished downloading models and saved to C:\\Users\\vanis\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd4b84f53794f6b8f5a0665d0d1950c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/res…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 11:27:32 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-05-18 11:27:32 INFO: Use device: cpu\n",
      "2022-05-18 11:27:32 INFO: Loading: tokenize\n",
      "2022-05-18 11:27:32 INFO: Loading: pos\n",
      "2022-05-18 11:27:32 INFO: Loading: lemma\n",
      "2022-05-18 11:27:32 INFO: Loading: depparse\n",
      "2022-05-18 11:27:32 INFO: Loading: sentiment\n",
      "2022-05-18 11:27:32 INFO: Loading: constituency\n",
      "2022-05-18 11:27:33 INFO: Loading: ner\n",
      "2022-05-18 11:27:33 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "data = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"Barack\",\n",
       "      \"lemma\": \"Barack\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"xpos\": \"NNP\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"nsubj:pass\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 6,\n",
       "      \"ner\": \"B-PERSON\",\n",
       "      \"multi_ner\": [\n",
       "        \"B-PERSON\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"Obama\",\n",
       "      \"lemma\": \"Obama\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"xpos\": \"NNP\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 1,\n",
       "      \"deprel\": \"flat\",\n",
       "      \"start_char\": 7,\n",
       "      \"end_char\": 12,\n",
       "      \"ner\": \"E-PERSON\",\n",
       "      \"multi_ner\": [\n",
       "        \"E-PERSON\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"was\",\n",
       "      \"lemma\": \"be\",\n",
       "      \"upos\": \"AUX\",\n",
       "      \"xpos\": \"VBD\",\n",
       "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"aux:pass\",\n",
       "      \"start_char\": 13,\n",
       "      \"end_char\": 16,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"born\",\n",
       "      \"lemma\": \"bear\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VBN\",\n",
       "      \"feats\": \"Tense=Past|VerbForm=Part|Voice=Pass\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 17,\n",
       "      \"end_char\": 21,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \"in\",\n",
       "      \"lemma\": \"in\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"IN\",\n",
       "      \"head\": 6,\n",
       "      \"deprel\": \"case\",\n",
       "      \"start_char\": 22,\n",
       "      \"end_char\": 24,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 6,\n",
       "      \"text\": \"Hawaii\",\n",
       "      \"lemma\": \"Hawaii\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"xpos\": \"NNP\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"obl\",\n",
       "      \"start_char\": 25,\n",
       "      \"end_char\": 31,\n",
       "      \"ner\": \"S-GPE\",\n",
       "      \"multi_ner\": [\n",
       "        \"S-GPE\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 7,\n",
       "      \"text\": \"?\",\n",
       "      \"lemma\": \"?\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \".\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 31,\n",
       "      \"end_char\": 32,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    }\n",
       "  ],\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"I\",\n",
       "      \"lemma\": \"I\",\n",
       "      \"upos\": \"PRON\",\n",
       "      \"xpos\": \"PRP\",\n",
       "      \"feats\": \"Case=Nom|Number=Sing|Person=1|PronType=Prs\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"nsubj:pass\",\n",
       "      \"start_char\": 33,\n",
       "      \"end_char\": 34,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"was\",\n",
       "      \"lemma\": \"be\",\n",
       "      \"upos\": \"AUX\",\n",
       "      \"xpos\": \"VBD\",\n",
       "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"aux:pass\",\n",
       "      \"start_char\": 35,\n",
       "      \"end_char\": 38,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"born\",\n",
       "      \"lemma\": \"bear\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VBN\",\n",
       "      \"feats\": \"Tense=Past|VerbForm=Part|Voice=Pass\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 39,\n",
       "      \"end_char\": 43,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"in\",\n",
       "      \"lemma\": \"in\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"IN\",\n",
       "      \"head\": 5,\n",
       "      \"deprel\": \"case\",\n",
       "      \"start_char\": 44,\n",
       "      \"end_char\": 46,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \"India\",\n",
       "      \"lemma\": \"India\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"xpos\": \"NNP\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"obl\",\n",
       "      \"start_char\": 47,\n",
       "      \"end_char\": 52,\n",
       "      \"ner\": \"S-GPE\",\n",
       "      \"multi_ner\": [\n",
       "        \"S-GPE\"\n",
       "      ]\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = data('Barack Obama was born in Hawaii? I was born in India')\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Sentence 1 token===========\n",
      "id: (1,)\ttext: Barack\n",
      "id: (2,)\ttext: Obama\n",
      "id: (3,)\ttext: was\n",
      "id: (4,)\ttext: born\n",
      "id: (5,)\ttext: in\n",
      "id: (6,)\ttext: Hawaii\n",
      "id: (7,)\ttext: ?\n",
      "==========Sentence 2 token===========\n",
      "id: (1,)\ttext: I\n",
      "id: (2,)\ttext: was\n",
      "id: (3,)\ttext: born\n",
      "id: (4,)\ttext: in\n",
      "id: (5,)\ttext: India\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "for i, sentences in enumerate(doc.sentences):\n",
    "  print(f'==========Sentence {i+1} token===========')\n",
    "  print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentences.tokens], sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack\n",
      "Obama\n",
      "was\n",
      "born\n",
      "in\n",
      "Hawaii\n",
      "?\n",
      "I\n",
      "was\n",
      "born\n",
      "in\n",
      "India\n"
     ]
    }
   ],
   "source": [
    "# work tokenization\n",
    "for word in doc.sentences:\n",
    "  for j in word.words:\n",
    "    print(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy_stanza\n",
      "  Downloading spacy_stanza-1.0.1-py3-none-any.whl (9.7 kB)\n",
      "Collecting stanza<1.4.0,>=1.2.0\n",
      "  Downloading stanza-1.3.0-py3-none-any.whl (432 kB)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy_stanza) (3.3.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza<1.4.0,>=1.2.0->spacy_stanza) (1.15.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza<1.4.0,>=1.2.0->spacy_stanza) (2.24.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza<1.4.0,>=1.2.0->spacy_stanza) (1.19.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza<1.4.0,>=1.2.0->spacy_stanza) (4.50.2)\n",
      "Requirement already satisfied: emoji in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza<1.4.0,>=1.2.0->spacy_stanza) (1.7.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza<1.4.0,>=1.2.0->spacy_stanza) (1.11.0)\n",
      "Requirement already satisfied: protobuf in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza<1.4.0,>=1.2.0->spacy_stanza) (3.20.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (2.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (0.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (1.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (8.0.15)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (2.11.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (0.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (20.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (3.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (50.3.1.post20201107)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (0.7.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_stanza) (0.6.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza<1.4.0,>=1.2.0->spacy_stanza) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza<1.4.0,>=1.2.0->spacy_stanza) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza<1.4.0,>=1.2.0->spacy_stanza) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza<1.4.0,>=1.2.0->spacy_stanza) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza<1.4.0,>=1.2.0->spacy_stanza) (3.7.4.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_stanza) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_stanza) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy_stanza) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.0.0->spacy_stanza) (5.2.1)\n",
      "Installing collected packages: stanza, spacy-stanza\n",
      "  Attempting uninstall: stanza\n",
      "    Found existing installation: stanza 1.4.0\n",
      "    Uninstalling stanza-1.4.0:\n",
      "      Successfully uninstalled stanza-1.4.0\n",
      "Successfully installed spacy-stanza-1.0.1 stanza-1.3.0\n"
     ]
    }
   ],
   "source": [
    "# combining advantages of model of stanza, functionality of spacy,\n",
    "# retreiving in stanza is difficult, not so with spacy\n",
    "\n",
    "!pip install spacy_stanza\n",
    "import spacy_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe38009c26534ab1b658b63029371cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/res…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 11:49:03 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-05-18 11:49:03 INFO: Use device: cpu\n",
      "2022-05-18 11:49:03 INFO: Loading: tokenize\n",
      "2022-05-18 11:49:03 INFO: Loading: pos\n",
      "2022-05-18 11:49:03 INFO: Loading: lemma\n",
      "2022-05-18 11:49:03 INFO: Loading: depparse\n",
      "2022-05-18 11:49:04 INFO: Loading: sentiment\n",
      "2022-05-18 11:49:04 INFO: Loading: constituency\n",
      "2022-05-18 11:49:04 INFO: Loading: ner\n",
      "2022-05-18 11:49:05 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# loading en model\n",
    "data = spacy_stanza.load_pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = data('Barack Obama was born in Hawaii? I was born in India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Barack PROPN NNP PERSON\n",
      "Obama Obama PROPN NNP PERSON\n",
      "was be AUX VBD \n",
      "born bear VERB VBN \n",
      "in in ADP IN \n",
      "Hawaii Hawaii PROPN NNP GPE\n",
      "? ? PUNCT . \n",
      "I I PRON PRP \n",
      "was be AUX VBD \n",
      "born bear VERB VBN \n",
      "in in ADP IN \n",
      "India India PROPN NNP GPE\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.lemma_, token.pos_, token.tag_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
